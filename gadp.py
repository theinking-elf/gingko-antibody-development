# -*- coding: utf-8 -*-
"""GADP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10mi9qbEFFUJ06_Iq303VKHRlJXEc2Qht
"""

# Install huggingface_hub
!pip install huggingface_hub
!huggingface-cli login

import pandas as pd

print("Downloading GDPa1 dataset from Hugging Face...")
df = pd.read_csv("hf://datasets/ginkgo-datapoints/GDPa1/GDPa1_v1.2_20250814.csv")

print(f"Dataset shape: {df.shape}")
print(f"\nColumns:")
print(df.columns.tolist())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import os
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from tqdm.auto import tqdm
import gc
from sklearn.model_selection import cross_val_predict

from google.colab import drive
drive.mount('/content/drive/')

gingko_path = '/content/drive/MyDrive/gingko antibody developability'

# Check how many missing values you have
print(f"Total rows before: {len(df)}")
print(f"Missing tonset_nanodsf: {df['Tonset'].isna().sum()}")

# Replace missing tonset_nanodsf values with 0 instead of dropping rows
df['Tonset'] = df['Tonset'].fillna(0)
df['Tm1'] = df['Tm1'].fillna(0)
df['Tm2'] = df['Tm2'].fillna(0)

# Verify that all missing values are now handled
print(f"Missing tonset_nanodsf after fill: {df['Tonset'].isna().sum()}")
print(f"Total rows after: {len(df)}")

# Reset index just to keep it clean
df = df.reset_index(drop=True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

!pip install fair-esm==2.0.0
import esm
from sklearn.tree import DecisionTreeRegressor

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Clear GPU memory
torch.cuda.empty_cache()
gc.collect()

# Load model
print("Loading ESM2 model...")
model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()
model = model.to(device)
model.eval()
batch_converter = alphabet.get_batch_converter()

# CRITICAL: Use smaller batch size
batch_size = 4  # Start with 4 instead of 32
print(f"Using batch size: {batch_size}")

embeddings = []
pbar = tqdm(total=len(df), desc="Processing", unit="seq")

for i in range(0, len(df), batch_size):
    try:
        batch_data = []
        for idx in range(i, min(i + batch_size, len(df))):
            sequence = df.iloc[idx]["vh_protein_sequence"]
            batch_data.append(("seq", sequence))

        batch_labels, batch_strs, batch_tokens = batch_converter(batch_data)
        batch_tokens = batch_tokens.to(device)

        with torch.no_grad():
            results = model(batch_tokens, repr_layers=[6])

        token_representations = results["representations"][6]
        batch_embeddings = token_representations.mean(1).cpu().numpy()
        embeddings.extend(batch_embeddings)

        # Clean up after each batch
        del batch_tokens, results, token_representations
        torch.cuda.empty_cache()

        pbar.update(len(batch_data))

    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"\n❌ OOM at batch {i}. Try even smaller batch size!")
            torch.cuda.empty_cache()
            break
        else:
            raise e

pbar.close()
print(f"✓ Generated {len(embeddings)} embeddings")

X = pd.DataFrame(embeddings)
y = df['Tm2']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Try multiple models
models = {
    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, verbose=0),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42, verbose=0)
}

print("\n" + "="*50)
print("MODEL EVALUATION")
print("="*50)

best_model = None
best_score = -np.inf

for name, model_obj in tqdm(models.items(), desc="Training models", unit="model"):
    # Train
    print(f"\nTraining {name}...")
    model_obj.fit(X_train, y_train)

    # Predictions
    train_pred = model_obj.predict(X_train)
    test_pred = model_obj.predict(X_test)

    # Metrics
    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    test_mae = mean_absolute_error(y_test, test_pred)

    print(f"\n{name}:")
    print(f"  Train R²: {train_r2:.4f}")
    print(f"  Test R²:  {test_r2:.4f}")
    print(f"  Test RMSE: {test_rmse:.4f}")
    print(f"  Test MAE:  {test_mae:.4f}")

    # Cross-validation with progress
    print(f"  Running 5-fold CV...")
    cv_scores = cross_val_score(model_obj, X_train, y_train, cv=5,
                                 scoring='r2', n_jobs=-1)
    print(f"  5-Fold CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

    if test_r2 > best_score:
        best_score = test_r2
        best_model = model_obj

print(f"\n{'='*50}")
print(f"Best model: {[k for k,v in models.items() if v == best_model][0]}")
print(f"Best test R²: {best_score:.4f}")
print(f"{'='*50}")

# Visualization
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.scatter(y_train, best_model.predict(X_train), alpha=0.5)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)
plt.xlabel('Actual Tm2')
plt.ylabel('Predicted Tm2')
plt.title('Training Set')
plt.grid(alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(y_test, best_model.predict(X_test), alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Tm2')
plt.ylabel('Predicted Tm2')
plt.title('Test Set')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

from sklearn.model_selection import PredefinedSplit, cross_val_predict
import pandas as pd

# Make sure complete_antibody has the fold column
# (merge it from the HF dataset if needed)

# Use the predefined folds for CV predictions
fold_column = df['hierarchical_cluster_IgG_isotype_stratified_fold'].values
cv_splits = PredefinedSplit(fold_column)

# Generate CV predictions (this is what you submit)
print("\nGenerating cross-validated predictions for submission...")
cv_predictions = cross_val_predict(best_model, X, y, cv=cv_splits, n_jobs=-1)

# Create submission dataframe
results_df = pd.DataFrame({
    'antibody_name': df['antibody_name'].values,
    'Tm2': cv_predictions,
    'hierarchical_cluster_IgG_isotype_stratified_fold': fold_column
})

# Group by antibody
results_df_aggregated = results_df.groupby('antibody_name').agg({
    'Tm2': 'mean',
    'hierarchical_cluster_IgG_isotype_stratified_fold': 'first'
}).reset_index()

print(f"\nUnique antibodies: {len(results_df_aggregated)}")
print(f"Fold distribution:")
print(results_df_aggregated['hierarchical_cluster_IgG_isotype_stratified_fold'].value_counts().sort_index())


# Save for submission
results_df_aggregated.to_csv('GDPa1 Cross-Validation Predictions.csv', index=False)
print("\n✓ Saved to 'GDPa1 Cross-Validation Predictions.csv'")

from google.colab import files
files.download('GDPa1 Cross-Validation Predictions.csv')

import pandas as pd
import numpy as np
import torch
import esm
from tqdm import tqdm
import gc

# Load your new data
test = os.path.join(gingko_path,'private test sequences.csv')
new_data = pd.read_csv(test)  # Should have 'antibody_name' and 'vh' columns

# Clear GPU memory
torch.cuda.empty_cache()
gc.collect()

# Load the SAME model you used for training
print("Loading ESM2 model...")
model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()
model = model.to(device)
model.eval()
batch_converter = alphabet.get_batch_converter()

# Use the same batch size
batch_size = 4
print(f"Using batch size: {batch_size}")

# Generate embeddings for new data
new_embeddings = []
pbar = tqdm(total=len(new_data), desc="Processing new data", unit="seq")

for i in range(0, len(new_data), batch_size):
    try:
        batch_data = []
        for idx in range(i, min(i + batch_size, len(new_data))):
            sequence = new_data.iloc[idx]["vh_protein_sequence"]  # Use 'vh' column name
            batch_data.append(("seq", sequence))

        batch_labels, batch_strs, batch_tokens = batch_converter(batch_data)
        batch_tokens = batch_tokens.to(device)

        with torch.no_grad():
            results = model(batch_tokens, repr_layers=[6])

        token_representations = results["representations"][6]
        batch_embeddings = token_representations.mean(1).cpu().numpy()
        new_embeddings.extend(batch_embeddings)

        # Clean up after each batch
        del batch_tokens, results, token_representations
        torch.cuda.empty_cache()

        pbar.update(len(batch_data))

    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"\n❌ OOM at batch {i}. Try even smaller batch size!")
            torch.cuda.empty_cache()
            break
        else:
            raise e

pbar.close()
print(f"✓ Generated {len(new_embeddings)} embeddings")

# Convert to numpy array
X_new = np.array(new_embeddings)

# Make predictions using your trained model
print("\nGenerating predictions for new data...")
new_predictions = best_model.predict(X_new)

# Create submission dataframe with only antibody_name and Tm2
submission_df = pd.DataFrame({
    'antibody_name': new_data['antibody_name'].values,
    'Tm2': new_predictions
})

# Aggregate if multiple rows per antibody
submission_df = submission_df.groupby('antibody_name').agg({
    'Tm2': 'mean'
}).reset_index()

print(f"\nPredictions for {len(submission_df)} unique antibodies")
print(submission_df.head())

# Save predictions
submission_df.to_csv('Private Test Set Predictions.csv', index=False)
print("\n✓ Saved to 'Private Test Set Predictions.csv'")

from google.colab import files
files.download('Private Test Set Predictions.csv')